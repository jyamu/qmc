/*! \page runningP Running QMCPACK

\section mpiopenmpS Parallelism using MPI/OpenMP programming model

QMCPACK uses MPI/OpenMP hybrid method for the parallelization.  QMC simulations
can always gain by using more parallel processing units and therefore MPI and
OpenMP parallelization is enabled by default, unless the programming
environment does not support MPI or OpenMP.

The optimal choice of MPI nodes and OpenMP threads depends on
- how much memory one needs to store read-only one-body orbitals
- memory hierarchy.

For the common multicore chips of today, using one MPI per node and setting
<c>OMP_NUM_THREADS</c> to the number of cores of a node will work. But, if a
walker can be fit into a NUMA node, it is better to use one MPI per NUMA node.
For instance, on Cray XC30 with dual octa-core Intel chips, setting
<c>OMP_NUM_THREADS=8</c> will work best.

Setting the number of walkers and samples can be tricky with parallel runs. The
basic rule is at least one walker per thread.  See Setting samples to learn how
to control QMC runs.

\section restartS Restarting from a previous QMC run

To restart a QMC run, these two steps have to be taken.

First, set <c>qmc/\@checkpoint</c> to the number of blocks between the
checkpoint. The default is  <c>qmc/\@checkpoint="-1"</c> and no data will be
stored in a <c>*.config.h5</c> file.
\code
<qmc method="dmc" checkpoint="10">
....
</qmc>
\endcode

The configuration file contains 
- configuration of all the walkers at the moment of checkpointing
- final state of random-number generators

Second, the main QMCPACK input file should contain, a <c>mcwalkerset</c> node prior to any
<c>qmc</c> sections. E.g.,
\code
<mcwalkerset fileroot="CONFIGH5" node="-1" nprocs="2" version="0 6" collected="yes"/>
<qmc method="dmc">
.....
</qmc>
\endcode

When a QMCPACK run is completed, <c>*.cont.xml</c> is written by the
application.  It should have <c>mcwalkerset</c> node based on the project ID
and sequence number according to the <c>qmc</c> blocks that have been executed.

Make sure to remove any unnecessary <c>qmc</c> sections or parameters, e.g.
<c>warmupsteps</c> has to be removed, unless a new warm-up block have to be
executed.

The configuration is "gathered" to the root node of a MPI group and the
attributes such as <c>mcwalkerset/\@node</c> and <c>mcwalkerset/\@nproces</c>
are mostly to guide a run but will be overwritten at the run time.

If continuing random-number generators is critical, it is important to use the
same number of parallel processing units, i.e. MPI tasks x OpenMp threads.  If
they are not the same between runs, new random generators will be used for the
restart run.

*/
